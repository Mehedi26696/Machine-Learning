{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d592b56-5579-478a-b6ec-a88d0ff539ad",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:green\">Logistic Regression</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4793dc-de06-4923-b881-85d40d1d20cc",
   "metadata": {},
   "source": [
    "# 1. Introduction to Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical model used for binary classification problems, where the target variable has two possible outcomes (e.g., 0 or 1, win or loss, pass or fail). \n",
    "It is used to model the probability of a binary outcome based on one or more predictor variables. Despite its name, logistic regression is a classification algorithm, not a regression algorithm.\n",
    "\n",
    "\n",
    "# 2. Hypothesis Function in Logistic Regression\n",
    "\n",
    "The hypothesis in logistic regression represents the probability that the target variable belongs to a particular class. It uses the sigmoid function to map the output to a probability value between 0 and 1.\n",
    "\n",
    "The sigmoid function, $\\sigma(z)$, is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( z \\) is a linear combination of the input features: $ z = w \\cdot X + b $\n",
    "- \\( w \\) is the weight vector\n",
    "- \\( X \\) is the feature vector\n",
    "- \\( b \\) is the bias term\n",
    "\n",
    "Thus, the hypothesis function in logistic regression is:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(X) = \\sigma(w \\cdot X + b) = \\frac{1}{1 + e^{-(w \\cdot X + b)}}\n",
    "$$\n",
    "\n",
    "This function outputs a probability between 0 and 1, representing the likelihood that the input \\( X \\) belongs to the positive class.\n",
    "\n",
    "\n",
    "# 3. Cost Function\n",
    "\n",
    "Logistic regression uses Log-Loss or Binary Cross-Entropy Loss as the cost function, which measures how well the model's predictions match the actual labels.\n",
    "\n",
    "The cost function for logistic regression is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left( y^{(i)} \\log(h_{\\theta}(X^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(X^{(i)})) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ m $ is the number of training examples\n",
    "- $ y^{(i)} $ is the actual label of the $ i $-th training example\n",
    "- $ h_{\\theta}(X^{(i)}) $ is the predicted probability for the $ i $-th example\n",
    "\n",
    "The goal of logistic regression is to minimize this cost function using optimization algorithms such as Gradient Descent.\n",
    "\n",
    "# 4. Confusion Matrix\n",
    "\n",
    "For a binary classification problem:\n",
    "\n",
    "|                | **Predicted Positive** | **Predicted Negative** |\n",
    "|----------------|-------------------------|-------------------------|\n",
    "| **Actual Positive** | True Positive (TP)       | False Negative (FN)       |\n",
    "| **Actual Negative** | False Positive (FP)      | True Negative (TN)        |\n",
    "\n",
    "1. **True Positive (TP):** Correctly predicted positive instances.\n",
    "2. **True Negative (TN):** Correctly predicted negative instances.\n",
    "3. **False Positive (FP):** Instances where the model incorrectly predicted positive when it was actually negative (Type I error).\n",
    "4. **False Negative (FN):** Instances where the model incorrectly predicted negative when it was actually positive (Type II error).\n",
    "### Evaluation Metrics:\n",
    "Using the confusion matrix, the following metrics can be calculated:\n",
    "\n",
    "1. **Accuracy:**  \n",
    "   The proportion of correctly classified instances out of the total number of instances.  \n",
    "   $$ \n",
    "   \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} \n",
    "   $$\n",
    "\n",
    "2. **Precision:**  \n",
    "   The proportion of true positive predictions out of all positive predictions.  \n",
    "   $$ \n",
    "   \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \n",
    "   $$\n",
    "\n",
    "3. **Recall (Sensitivity or True Positive Rate):**  \n",
    "   The proportion of actual positives correctly identified.  \n",
    "   $$ \n",
    "   \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \n",
    "   $$\n",
    "\n",
    "4. **F1 Score:**  \n",
    "   The harmonic mean of precision and recall.  \n",
    "   $$ \n",
    "   \\text{F1 Score} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \n",
    "   $$\n",
    "\n",
    "5. **False Positive Rate (FPR):**  \n",
    "   The proportion of actual negatives incorrectly classified as positive.  \n",
    "   $$ \n",
    "   \\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}} \n",
    "   $$\n",
    "\n",
    "6. **Specificity (True Negative Rate):**  \n",
    "   The proportion of actual negatives correctly classified as negative.  \n",
    "   $$ \n",
    "   \\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} \n",
    "   $$\n",
    "# 5. Types of Logistic Regression\n",
    "\n",
    "1. Binary Logistic Regression: This is used for binary classification problems where the target variable has two classes (e.g., 0 or 1). The sigmoid function is used to model the probability of one class.\n",
    "\n",
    "2. Multinomial Logistic Regression: This is used for classification problems where the target variable has more than two classes. The softmax function is used in place of the sigmoid function to model the probability distribution over all classes.\n",
    "\n",
    "   The softmax function for K classes is:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\quad \\text{for } j = 1 \\text{ to } K\n",
    "$$\n",
    "\n",
    "Where $ z_i $ is the score for class $ i $ and $ K $ is the total number of classes.\n",
    "\n",
    "The probability for class $ c $ is:\n",
    "\n",
    "$$\n",
    "P(Y = c | X) = \\frac{e^{w_c \\cdot X + b_c}}{\\sum_{k=1}^{K} e^{w_k \\cdot X + b_k}} \\quad \\text{for } k = 1 \\text{ to } K\n",
    "$$\n",
    "\n",
    "\n",
    "# 6. Proof of Sigmoid Function\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "This function maps any real-valued input to a value between 0 and 1. \n",
    "\n",
    "Proof that the sigmoid function maps to (0, 1):\n",
    "\n",
    "- As $ z \\to \\infty $, $ \\sigma(z) \\to 1 $\n",
    "- As $ z \\to -\\infty $, $ \\sigma(z) \\to 0 $\n",
    "\n",
    "Thus, the sigmoid function always outputs values in the interval (0, 1), making it suitable for modeling probabilities in logistic regression.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87fb4e70-0d21-4efa-9fc8-2f9d2b5189e5",
   "metadata": {},
   "source": [
    "<img src=\"1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675be78-5add-4ceb-9f4b-20845047147c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
