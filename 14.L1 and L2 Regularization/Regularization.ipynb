{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4769dddb-5372-4653-92b4-0d08cc0418a7",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:purple\">Regularization in Machine Learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fed3e0-6080-4da1-b5e6-992d59e8c6bf",
   "metadata": {},
   "source": [
    "---\n",
    "Regularization is a vital technique in machine learning used to address **overfitting** and improve a model's **generalization ability**. It achieves this by adding a penalty term to the loss function, discouraging complex models that may overfit the training data. By constraining model parameters, regularization helps find a balance between **bias** and **variance** for optimal performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Overfitting and Underfitting**\n",
    "- **Overfitting**: Occurs when a model learns the noise in training data, leading to poor generalization on unseen data. Symptoms include high training accuracy and low testing accuracy.\n",
    "- **Underfitting**: Happens when a model fails to capture the patterns in the training data, resulting in poor performance on both training and testing datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **Bias-Variance Tradeoff**\n",
    "- **Bias**: Error due to overly simplistic models that fail to capture underlying patterns (underfitting).\n",
    "- **Variance**: Error caused by overly complex models that memorize data noise (overfitting).\n",
    "- **Goal**: Find the right balance between bias and variance for consistent and accurate predictions on new data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Regularization Techniques**\n",
    "\n",
    "#### 1. **Lasso Regularization (L1 Regularization)**\n",
    "- Adds the absolute value of coefficients as a penalty term to the loss function.\n",
    "- Promotes **sparse solutions**, reducing some feature coefficients to zero for **feature selection**.\n",
    "- **Cost Function**:\n",
    "  $$\n",
    "  \\text{Cost} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{i=1}^m |w_i|\n",
    "  $$\n",
    "  - **Variables**:\n",
    "    - *$n$*: Number of examples\n",
    "    - *$m$*: Number of features\n",
    "    - *$y_i$*: Actual target value for the $i\\text{th}$ example\n",
    "    - *$\\hat{y}_i$*: Predicted target value for the $i\\text{th}$ example\n",
    "    - *$w_i$*: Coefficients/weights of the features\n",
    "    - *$\\lambda$*: Regularization strength\n",
    "\n",
    "#### 2. **Ridge Regularization (L2 Regularization)**\n",
    "- Adds the squared magnitude of coefficients as a penalty term to the loss function.\n",
    "- Reduces coefficient sensitivity and handles **multicollinearity** effectively.\n",
    "- **Cost Function**:\n",
    "  $$\n",
    "  \\text{Cost} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{i=1}^m w_i^2\n",
    "  $$\n",
    "  - **Variables**:\n",
    "    - *$n$*: Number of examples\n",
    "    - *$m$*: Number of features\n",
    "    - *$y_i$*: Actual target value for the $i\\text{th}$ example\n",
    "    - *$\\hat{y}_i$*: Predicted target value for the $i\\text{th}$ example\n",
    "    - *$w_i$*: Coefficients/weights of the features\n",
    "    - *$\\lambda$*: Regularization strength\n",
    "\n",
    "#### 3. **Elastic Net Regularization**\n",
    "- Combines L1 and L2 regularization, controlled by an additional hyperparameter (\\(\\alpha\\)).\n",
    "- Balances feature selection (L1) and coefficient shrinkage (L2).\n",
    "- **Cost Function**:\n",
    "  $$\n",
    "  \\text{Cost} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda \\left( (1 - \\alpha) \\sum_{i=1}^m |w_i| + \\alpha \\sum_{i=1}^m w_i^2 \\right)\n",
    "  $$\n",
    "  - **Variables**:\n",
    "    - *$n$*: Number of examples\n",
    "    - *$m$*: Number of features\n",
    "    - *$y_i$*: Actual target value for the $i\\text{th}$ example\n",
    "    - *$\\hat{y}_i$*: Predicted target value for the $i\\text{th}$ example\n",
    "    - *$w_i$*: Coefficients/weights of the features\n",
    "    - *$\\lambda$*: Regularization strength\n",
    "    - *$\\alpha$*: Hyperparameter controlling the balance between L1 and L2 regularization\n",
    "\n",
    "---\n",
    "\n",
    "### **Benefits of Regularization**\n",
    "1. **Improves Generalization**: Reduces overfitting by focusing on the underlying patterns rather than noise.\n",
    "2. **Feature Selection**: L1 regularization simplifies models by eliminating irrelevant features.\n",
    "3. **Stabilizes Models**: Reduces sensitivity to data changes and ensures consistent performance across datasets.\n",
    "4. **Handles Multicollinearity**: Controls the magnitudes of correlated coefficients.\n",
    "5. **Enhances Performance**: Prevents excessive weighting of irrelevant features or outliers.\n",
    "6. **Adjustable Complexity**: Hyperparameters *$\\lambda$* or *$\\alpha$* allow fine-tuning the balance between bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Choosing Regularization Techniques**\n",
    "- Use **Lasso** for feature selection or when working with sparse datasets.\n",
    "- Use **Ridge** for handling multicollinearity and ensuring stability.\n",
    "- Use **Elastic Net** when a mix of L1 and L2 regularization is beneficial.\n",
    "\n",
    "---\n",
    "\n",
    "By applying regularization thoughtfully, machine learning practitioners can build robust models that generalize well across diverse datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e07b2-b918-4336-95b4-8cd0b05d9609",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" width=400> <img src=\"2.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718e741-5bb3-4060-b1b5-f6113aa6b79a",
   "metadata": {},
   "source": [
    "<img src=\"3.png\" width=400> <img src=\"4.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac4d3c-1192-4af7-aecc-dbb5238df048",
   "metadata": {},
   "source": [
    "<img src=\"5.png\" width=400>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
