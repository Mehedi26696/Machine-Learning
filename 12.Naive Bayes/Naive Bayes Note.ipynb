{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef21f03-ec35-49db-bc51-a1a43afd9e97",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:green\">Naive Bayes</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee51ff8-1fda-4e5f-839e-2933a1f37f2e",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "\n",
    "Naive Bayes classifiers are a group of probabilistic algorithms based on **Bayes' Theorem**, used primarily for classification tasks. Despite the \"naive\" assumption of conditional independence between features, these classifiers are popular due to their simplicity, efficiency, and effectiveness in various applications.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points About Naive Bayes\n",
    "\n",
    "1. **Probabilistic Nature**:\n",
    "   - Predicts the probability of a data instance belonging to a class given feature values.\n",
    "   - Relies on **Bayes' Theorem** for updating probabilities based on new data.\n",
    "\n",
    "2. **Naivety Explained**:\n",
    "   - Assumes that features are **conditionally independent** given the class label.\n",
    "   - While this assumption rarely holds in real-world data, it simplifies computation and often yields effective results.\n",
    "\n",
    "3. **Applications**:\n",
    "   - **Spam detection**: Identifying unsolicited emails.\n",
    "   - **Sentiment analysis**: Categorizing text as positive, negative, or neutral.\n",
    "   - **Recommendation systems**: Predicting user preferences.\n",
    "   - **Text classification**: Handling high-dimensional data like bag-of-words models in NLP.\n",
    "\n",
    "4. **Strengths**:\n",
    "   - **Fast**: Works well with large datasets and high-dimensional features.\n",
    "   - **Simple to implement**: Requires fewer parameters compared to other complex models.\n",
    "\n",
    "5. **Weaknesses**:\n",
    "   - Over-simplifies relationships between features.\n",
    "   - Assumes all features are equally important, which may not always be true.\n",
    "\n",
    "---\n",
    "\n",
    "### Bayes' Theorem in Machine Learning\n",
    "\n",
    "#### Introduction\n",
    "Bayes' Theorem finds the probability of an event occurring given the probability of another event that has already occurred. It is stated mathematically as:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **P(A|B)**: Posterior probability - Probability of event A given B (evidence).\n",
    "- **P(B|A)**: Likelihood - Probability of B given A.\n",
    "- **P(A)**: Prior probability - Initial probability of A.\n",
    "- **P(B)**: Marginal probability - Probability of B.\n",
    "\n",
    "#### Application in Machine Learning\n",
    "In the context of classification problems, Bayes' Theorem can be expressed as:\n",
    "\n",
    "$$\n",
    "P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **y**: Class variable (target outcome).\n",
    "- **X**: Feature vector $(x_1, x_2, \\dots, x_n)$.\n",
    "- **P(y|X)**: Posterior probability - Probability of class y given features X.\n",
    "- **P(X|y)**: Likelihood - Probability of observing features X given class y.\n",
    "- **P(y)**: Prior probability of class y.\n",
    "- **P(X)**: Marginal probability of features X.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Breakdown\n",
    "\n",
    "### Example Dataset: Weather and Golf\n",
    "\n",
    "| Outlook  | Temperature | Humidity | Windy  | Play Golf |\n",
    "|----------|-------------|----------|--------|-----------|\n",
    "| Rainy    | Hot         | High     | False  | No        |\n",
    "| Overcast | Hot         | High     | False  | Yes       |\n",
    "\n",
    "- **Feature Matrix**: Attributes like `Outlook`, `Temperature`, etc.\n",
    "- **Response Vector**: Class labels like `Play Golf` (Yes/No).\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions of Naive Bayes\n",
    "\n",
    "1. **Feature Independence**:\n",
    "   - Each feature contributes independently to the class prediction.\n",
    "   - Example: \"Rainy\" and \"High humidity\" are treated as unrelated factors.\n",
    "\n",
    "2. **Feature Type Assumptions**:\n",
    "   - **Continuous Features**: Assumed to follow a normal distribution.\n",
    "   - **Discrete Features**: Modeled using multinomial distributions.\n",
    "\n",
    "3. **No Missing Data**:\n",
    "   - Requires complete datasets with no null values.\n",
    "\n",
    "4. **Equal Feature Importance**:\n",
    "   - All features are equally weighted in influencing the prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Naive Bayes is a straightforward and fast classification algorithm, especially effective for high-dimensional datasets like text. While its independence assumption is rarely true, it often delivers strong results in practice due to its simplicity and scalability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
