{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988ba897-efda-4004-82e8-7d1a01cce85d",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:green\">Gradient Descent</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928a0b3-d889-447c-9ecd-c6ace880a792",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "### Definition:\n",
    "- Gradient Descent (GD) is an optimization algorithm to minimize the cost function and find optimal model parameters (weights and biases).\n",
    "\n",
    "### Loss Function ($J$):\n",
    "- The cost function measures the difference between actual and predicted values:\n",
    "  $$\n",
    "  J(w, b) = \\frac{1}{n} \\sum_{i=1}^n (y_p - y_i)^2\n",
    "  $$\n",
    "  - **For a single data point:**\n",
    "    $$\n",
    "    J(w, b) = \\frac{1}{n} (y_p - y_i)^2\n",
    "    $$\n",
    "  - Here, $y_p$ is the predicted value, $y$ is the actual value, and $n$ is the number of data points.\n",
    "\n",
    "### Gradients:\n",
    "1. **With respect to weight ($w$):**\n",
    "   $$\n",
    "   J'_w = \\frac{\\partial J(w, b)}{\\partial w} =\\sum_{i=1}^n \\frac{2(y_p - y_i)}{n} \\cdot x_i\n",
    "   $$\n",
    "   - Gradient indicates the direction and magnitude of weight updates.\n",
    "2. **With respect to bias ($b$):**\n",
    "   $$\n",
    "   J'_b = \\frac{\\partial J(w, b)}{\\partial b} = \\sum_{i=1}^n \\frac{2(y_p - y_i)}{n}\n",
    "   $$\n",
    "   - This gradient adjusts the bias parameter.\n",
    "\n",
    "### How Gradient Descent Works:\n",
    "1. **Compute Gradients:**\n",
    "   - Calculate $J'_w$ and $J'_b$ for the current weights and biases.\n",
    "2. **Update Parameters:**\n",
    "   - Adjust weights and biases using the formula:\n",
    "     $\n",
    "     Param = Param - \\gamma \\nabla J\n",
    "     $\n",
    "     - $\\gamma$: Learning rate\n",
    "     - $\\nabla J$: Gradient of the loss function\n",
    "3. **Repeat:**\n",
    "   - Iterate until the loss function converges to a minimum.\n",
    "\n",
    "### Parameter Update Rule:\n",
    "- The updated weights and biases are:\n",
    "  $$\n",
    "  w = w - \\gamma J'_w\n",
    "  $$\n",
    "  $$\n",
    "  b = b - \\gamma J'_b\n",
    "  $$\n",
    "\n",
    "### Applications:\n",
    "- Used in linear regression, logistic regression, and neural networks.\n",
    "- Generalizable to multi-layered models and complex datasets.\n",
    "\n",
    "### Important Points:\n",
    "1. **Learning Rate ($\\gamma$):**\n",
    "   - Controls the step size during updates.\n",
    "   - Too small: Slow convergence.\n",
    "   - Too large: Risk of overshooting the minimum.\n",
    "2. **Convergence:**\n",
    "   - Stops when the loss function reaches a minimum or changes insignificantly.\n",
    "3. **Complex Models:**\n",
    "   - For neural networks, gradients are computed for all layers, and backpropagation is used to distribute updates.\n",
    "\n",
    "### Example in Context:\n",
    "- For linear regression:\n",
    "  - $$y_p = xw + b$$\n",
    "  - Gradients:\n",
    "    $$\n",
    "    J'_w = \\frac{2}{n}(y_p - y)x, \\quad J'_b = \\frac{2}{n}(y_p - y)\n",
    "    $$\n",
    "  - Update weights and biases iteratively to minimize $J(w, b)$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62a5b514-d42f-4659-acf8-7af01b48fdaf",
   "metadata": {},
   "source": [
    "<img src=\"1.png\">  <img src=\"2.png\"> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ddb59bb-664c-45ef-b918-d60827d76591",
   "metadata": {},
   "source": [
    "<img src=\"3.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
